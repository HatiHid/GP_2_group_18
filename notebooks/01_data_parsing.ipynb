{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **01_data_parsing.ipynb**\n",
        "\n",
        "В этом ноутбуке собираются данные о новостройках Москвы.\n",
        "\n",
        "Данные получены с помощью парсинга с сайта Move.ru.\n",
        "\n",
        "Результат ноутбука — таблица с сырыми объектами (цены, площади, адреса, девелопер и др.), которая используется в дальнейшей обработке и анализе."
      ],
      "metadata": {
        "id": "HBMZkjhyWCuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fa3saglnhDWE",
        "outputId": "e975186e-915f-4b06-ab9c-4c3aae1cfff3",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Collecting fake-useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (5.4.0)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.38.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Collecting trio<1.0,>=0.31.0 (from selenium)\n",
            "  Downloading trio-0.32.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket<1.0,>=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: websocket-client<2.0,>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from selenium) (1.9.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (1.2.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from webdriver-manager) (25.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (25.4.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (2.4.0)\n",
            "Collecting outcome (from trio<1.0,>=0.31.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from trio<1.0,>=0.31.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket<1.0,>=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from wsproto>=0.14->trio-websocket<1.0,>=0.12.2->selenium) (0.16.0)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading selenium-4.38.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading trio-0.32.0-py3-none-any.whl (512 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.0/512.0 kB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, fake-useragent, webdriver-manager, trio, trio-websocket, selenium\n",
            "Successfully installed fake-useragent-2.2.0 outcome-1.3.0.post0 selenium-4.38.0 trio-0.32.0 trio-websocket-0.12.2 webdriver-manager-4.0.2 wsproto-1.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 fake-useragent lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "from fake_useragent import UserAgent"
      ],
      "metadata": {
        "id": "U9ix8iHRGXTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала парсим главную страницу с предложенными объявлениями и смотрим где находятся ссылки, которые нам надо вычленить"
      ],
      "metadata": {
        "id": "dVeJuz3I5UwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://move.ru/kvartiry_v_novostroykah/v_predelah_mkad/'\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "5NPeMeBXGVCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "вычленяем эти самые ссылки с 400 страниц (в итоге вышло меньше, поскольку я по несколько раз продолжала парсить). На каждой странице по 30 ссылок"
      ],
      "metadata": {
        "id": "OANEkjSg5d4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем экземпляр UserAgent для генерации случайных заголовков браузера\n",
        "ua = UserAgent()\n",
        "\n",
        "def get_page_with_retry(url, max_retries=3):\n",
        "\n",
        "    # Цикл по количеству попыток\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Формируем заголовки HTTP-запроса\n",
        "            headers = {\n",
        "                'User-Agent': ua.random, # Случайный User-Agent\n",
        "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', # Типы контента, которые принимаем\n",
        "                'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3', # Предпочтительные языки\n",
        "                'Accept-Encoding': 'gzip, deflate, br', # Поддерживаемые методы сжатия\n",
        "                'Connection': 'keep-alive', # Поддержание соединения\n",
        "                'Upgrade-Insecure-Requests': '1', # Запрос на обновление небезопасных запросов\n",
        "            }\n",
        "            # Выполняем GET-запрос с заданными параметрами\n",
        "            response = requests.get(\n",
        "                url,\n",
        "                headers=headers,\n",
        "                timeout=10,\n",
        "                allow_redirects=True # Разрешаем перенаправления\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response\n",
        "            else:\n",
        "                print(f\"Попытка {attempt + 1}: Статус код {response.status_code} для {url}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Попытка {attempt + 1}: Ошибка при запросе {url}: {e}\")\n",
        "\n",
        "        if attempt < max_retries - 1:\n",
        "            time.sleep(random.uniform(2, 5))\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_urls_from_page(soup, base_url):\n",
        "    urls = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        if re.search(r'/objects/[\\w-]+\\s*[\\w\\s]*\\d+', href):\n",
        "            if href not in urls:\n",
        "                urls.append(href)\n",
        "    return urls\n",
        "\n",
        "def main():\n",
        "    base_url = 'https://move.ru/kvartiry_v_novostroykah/v_predelah_mkad/'\n",
        "    all_urls = []\n",
        "    max_pages = 400\n",
        "\n",
        "    for page_num in range(1, max_pages + 1):\n",
        "        print(f\"Обрабатывается страница {page_num} из {max_pages}...\")\n",
        "\n",
        "        # Формируем URL страницы (для первой страницы без параметра page)\n",
        "        if page_num == 1:\n",
        "            page_url = base_url\n",
        "        else:\n",
        "            page_url = f\"{base_url}?page={page_num}\"\n",
        "\n",
        "        # Получаем страницу\n",
        "        response = get_page_with_retry(page_url)\n",
        "\n",
        "        if response is None:\n",
        "            print(f\"Не удалось получить страницу {page_num}, пропускаем...\")\n",
        "            continue\n",
        "\n",
        "        # Парсим HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Извлекаем URL\n",
        "        page_urls = extract_urls_from_page(soup, base_url)\n",
        "        all_urls.extend(page_urls)\n",
        "\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "\n",
        "    all_urls = list(set(all_urls))\n",
        "\n",
        "    with open('extracted_urls.txt', 'w', encoding='utf-8') as f:\n",
        "        for url in all_urls:\n",
        "            f.write(url + '\\n')\n",
        "\n",
        "    print(f\"Результаты сохранены в файл 'extracted_urls.txt'\")\n",
        "\n",
        "if name == \"main\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "6Plx67XA3B-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Открываем сохраненный файл с ссылками"
      ],
      "metadata": {
        "id": "yenewzZv8eTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('extracted_urls.txt', 'r', encoding='utf-8') as file:\n",
        "    urls = [line.strip() for line in file if line.strip()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yDuo8HRucbi",
        "outputId": "4bb59247-d105-493c-c981-b5c541824374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найдено 10496 ссылок для парсинга\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь парсим одно из объявлений и смотрим его код, где находятся признаки, которые мы хотимм"
      ],
      "metadata": {
        "id": "tw7JNYFp8qV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://move.ru/objects/prodaetsya_2-komnatnaya_kvartira_ploschadyu_604_kvm_moskva_mojayskiy_rayon_ulica_vereyskaya_d_29s35_9285880157/'\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "QVyvF2qtTHqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Создаем сессию для сохранения cookies и заголовков между запросами\n",
        "session = requests.Session()\n",
        "# Устанавливаем стандартные заголовки для имитации браузера\n",
        "session.headers.update({\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "})\n",
        "\n",
        "# Предкомпилируем регулярные выражения\n",
        "regex_patterns = {\n",
        "    'id': re.compile(r'_(\\d+)/?$'),  # _ затем одна/несколько цифр, затем необязательный / и конец строки $\n",
        "    'price': re.compile(r'[\\d\\s\\xa0]+'), # один/несколько символов из класса: цифры, пробелы, неразрывные пробелы\n",
        "    'year': re.compile(r'(\\d{4})'), #точно 4 цифры подряд\n",
        "    'rooms': re.compile(r'(\\d+)'), # одна/несколько цифр\n",
        "    'area': re.compile(r'(\\d+(?:\\.\\d+)?)'), # цифры, затем необязательная незахватываемая группа (?:) с точкой и цифрами\n",
        "    'floor_current': re.compile(r'(\\d+)/\\d+'), # цифры, затем /, затем цифры\n",
        "    'floor_total': re.compile(r'/(\\d+)'), # символ /, затем цифры\n",
        "    'views': re.compile(r'(\\d+)'), # одна/несколько цифр\n",
        "    'distance_km': re.compile(r'([\\d,]+)\\s*км'), # цифры/запятые, затем пробелы, затем текст км\n",
        "    'distance_m': re.compile(r'(\\d+)\\s*м') # цифры, затем пробелы, затем текст м\n",
        "}\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for i, url in enumerate(urls, 1):\n",
        "    try:\n",
        "        if i%5 == 0:\n",
        "            print(f\"Парсим {i}/{len(urls)}\")\n",
        "\n",
        "        # Выполняем GET-запрос к странице объявления\n",
        "        response = session.get(url, timeout=8)\n",
        "        # Парсим HTML-контент с помощью BeautifulSoup и lxml парсера\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "        data = {}\n",
        "\n",
        "        # 1. Айди\n",
        "        id_match = regex_patterns['id'].search(url)\n",
        "        data['id'] = id_match.group(1) if id_match else None\n",
        "\n",
        "        # 2. Цена в миллионах\n",
        "        price_elem = soup.find('span', class_='card-objects-price__main-price')\n",
        "        if price_elem:\n",
        "            price_text = price_elem.get_text(strip=True)\n",
        "            # Убираем все нецифровые символы кроме точек и запятых\n",
        "            clean_price = re.sub(r'[^\\d,]', '', price_text.replace('\\xa0', ''))\n",
        "            clean_price = clean_price.replace(',', '.')\n",
        "            if clean_price:\n",
        "                try:\n",
        "                    price = float(clean_price)\n",
        "                    data['price_millions'] = round(price / 1000000, 2)\n",
        "                except ValueError:\n",
        "                    print(f\"Не удалось преобразовать цену: {price_text}\")\n",
        "\n",
        "        # 3-5. Название ЖК + застройщик + класс жилья\n",
        "        spec_items = soup.find_all('div', class_='card-specifications-table__item')\n",
        "        count = 0\n",
        "        for item in spec_items:\n",
        "            desc_elem = item.find('span', class_='card-specifications-table__description')\n",
        "            value_elem = item.find('span', class_='card-specifications-table__title')\n",
        "            link_elem = item.find('a', class_='card-specifications-table__link')\n",
        "\n",
        "            # если мы нашли все три признака заканчиваем и не обрабатываем дальше\n",
        "            if count == 3:\n",
        "                  break\n",
        "\n",
        "            if desc_elem:\n",
        "                desc_text = desc_elem.get_text(strip=True)\n",
        "\n",
        "                # Название ЖК\n",
        "                if desc_text == 'Название ЖК':\n",
        "                  data['complex_name'] = link_elem.get_text(strip=True)\n",
        "                  count += 1\n",
        "\n",
        "                # Застройщик\n",
        "                elif desc_text == 'Застройщик':\n",
        "                    data['developer'] = link_elem.get_text(strip=True)\n",
        "                    count += 1\n",
        "\n",
        "                # Класс жилья\n",
        "                elif desc_text == 'Класс жилья':\n",
        "                    data['housing_class'] = value_elem.get_text(strip=True)\n",
        "                    count += 1\n",
        "\n",
        "\n",
        "        # 6-12. Основные данные (площади, этажи, комнаты)\n",
        "        table_items = soup.find_all('div', class_='card-specifications-table__item')\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        for item in table_items:\n",
        "            desc_elem = item.find('span', class_='card-specifications-table__description')\n",
        "            title_elem = item.find('span', class_='card-specifications-table__title')\n",
        "\n",
        "             # если мы нашли все пять признаков, заканчиваем и не обрабатываем дальше\n",
        "            if count == 5:\n",
        "              break\n",
        "\n",
        "            if desc_elem and title_elem:\n",
        "                desc = desc_elem.get_text(strip=True)\n",
        "                title = title_elem.get_text(strip=True)\n",
        "\n",
        "                if 'Общая площадь' in desc and 'total_area' not in data:\n",
        "                    match = regex_patterns['area'].search(title)\n",
        "                    if match:\n",
        "                        data['total_area'] = float(match.group(1))\n",
        "                        count += 1\n",
        "                elif 'Жилая площадь' in desc:\n",
        "                    match = regex_patterns['area'].search(title)\n",
        "                    if match:\n",
        "                        data['living_area'] = float(match.group(1))\n",
        "                        count += 1\n",
        "                elif 'Площадь кухни' in desc:\n",
        "                    match = regex_patterns['area'].search(title)\n",
        "                    if match:\n",
        "                        data['kitchen_area'] = float(match.group(1))\n",
        "                        count += 1\n",
        "                elif 'Этаж' in desc and 'floor' not in data:\n",
        "                    match = regex_patterns['floor_current'].search(title)\n",
        "                    if match:\n",
        "                        data['floor'] = int(match.group(1))\n",
        "                    match = regex_patterns['floor_total'].search(title)\n",
        "                    if match:\n",
        "                        data['total_floors'] = int(match.group(1))\n",
        "                    count += 1\n",
        "                elif 'Количество комнат' in desc and 'rooms' not in data:\n",
        "                    match = regex_patterns['rooms'].search(title)\n",
        "                    if match:\n",
        "                        data['rooms'] = int(match.group(1))\n",
        "                        count += 1\n",
        "\n",
        "        # 13. Полный адрес\n",
        "        address_link = soup.find('a', class_='base-link card-objects-location__address-link')\n",
        "        data['full_address'] = address_link['title'] if address_link and address_link.get('title') else None\n",
        "\n",
        "        # 14-17. Метро\n",
        "        metro_data = []\n",
        "        metro_stations = soup.find_all('li', class_='card-objects-near-stations__station')\n",
        "\n",
        "        for station in metro_stations:\n",
        "            name_elem = station.find('a', class_='card-objects-near-stations__station-link')\n",
        "            duration_elem = station.find('span', class_='card-objects-near-stations__station-duration')\n",
        "            distance_elem = station.find('span', class_='card-objects-near-stations__station-distance')\n",
        "\n",
        "            if name_elem:\n",
        "                metro_name = name_elem.get_text(strip=True)\n",
        "\n",
        "                # Парсим расстояние\n",
        "                distance_m = None\n",
        "                if distance_elem:\n",
        "                    distance_text = distance_elem.get_text(strip=True)\n",
        "                    km_match = regex_patterns['distance_km'].search(distance_text)\n",
        "                    if km_match:\n",
        "                        distance_m = int(float(km_match.group(1).replace(',', '.')) * 1000) # group(1) это то, что было захвачено в первых скобках ([\\d,]+) регулярного выражения\n",
        "                    else:\n",
        "                        m_match = regex_patterns['distance_m'].search(distance_text)\n",
        "                        if m_match:\n",
        "                            distance_m = int(m_match.group(1))\n",
        "\n",
        "                metro_data.append({\n",
        "                    'name': metro_name,\n",
        "                    'distance_m': distance_m\n",
        "                })\n",
        "\n",
        "        # Сортируем по расстоянию и берем ближайшее\n",
        "        if metro_data:\n",
        "            metro_data.sort(key=lambda x: x['distance_m'] if x['distance_m'] else float('inf'))\n",
        "            nearest = metro_data[0]\n",
        "\n",
        "            data['metro_data'] = metro_data\n",
        "            data['metro_names'] = [m['name'] for m in metro_data]\n",
        "            data['nearest_metro'] = nearest['name']\n",
        "            data['nearest_metro_distance'] = nearest['distance_m']\n",
        "\n",
        "        # 18. Просмотры\n",
        "        views_elems = soup.find_all('span', class_='card-meta__item')\n",
        "        for elem in views_elems:\n",
        "            views_text = elem.get_text(strip=True)\n",
        "            if 'просмотр' in views_text.lower():\n",
        "                views_match = regex_patterns['views'].search(views_text)\n",
        "                if views_match:\n",
        "                    data['views'] = int(views_match.group(1))\n",
        "                    break\n",
        "\n",
        "        data['url'] = url\n",
        "        all_data.append(data)\n",
        "        time.sleep(random.uniform(0.1, 0.5))\n",
        "        if i % 100 == 0:\n",
        "            time.sleep(random.uniform(1.5, 2))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при парсинге {url}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "print(f\"\\nУспешно собрано {len(df)} объявлений\")\n",
        "\n",
        "# Сохраняем в CSV\n",
        "df.to_csv('parsingMOVE.csv', index=False, encoding='utf-8')\n",
        "print(\"\\nДанные сохранены в parsingMOVE.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E60xLuXA9G3X",
        "outputId": "b3e656f0-cf12-4f70-d23f-c6c3605c44c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найдено 10496 ссылок для парсинга\n",
            "Парсим 5/1000\n",
            "Парсим 10/1000\n",
            "Парсим 15/1000\n",
            "Парсим 20/1000\n",
            "Парсим 25/1000\n",
            "Парсим 30/1000\n",
            "Парсим 35/1000\n",
            "Парсим 40/1000\n",
            "Парсим 45/1000\n",
            "Парсим 50/1000\n",
            "Парсим 55/1000\n",
            "Парсим 60/1000\n",
            "Парсим 65/1000\n",
            "Парсим 70/1000\n",
            "Парсим 75/1000\n",
            "Парсим 80/1000\n",
            "Парсим 85/1000\n",
            "Парсим 90/1000\n",
            "Парсим 95/1000\n",
            "Парсим 100/1000\n",
            "Парсим 105/1000\n",
            "Парсим 110/1000\n",
            "Парсим 115/1000\n",
            "Парсим 120/1000\n",
            "Парсим 125/1000\n",
            "Парсим 130/1000\n",
            "Парсим 135/1000\n",
            "Парсим 140/1000\n",
            "Парсим 145/1000\n",
            "Парсим 150/1000\n",
            "Парсим 155/1000\n",
            "Парсим 160/1000\n",
            "Парсим 165/1000\n",
            "Парсим 170/1000\n",
            "Парсим 175/1000\n",
            "Парсим 180/1000\n",
            "Парсим 185/1000\n",
            "Парсим 190/1000\n",
            "Парсим 195/1000\n",
            "Парсим 200/1000\n",
            "Парсим 205/1000\n",
            "Парсим 210/1000\n",
            "Парсим 215/1000\n",
            "Парсим 220/1000\n",
            "Парсим 225/1000\n",
            "Парсим 230/1000\n",
            "Парсим 235/1000\n",
            "Парсим 240/1000\n",
            "Парсим 245/1000\n",
            "Парсим 250/1000\n",
            "Парсим 255/1000\n",
            "Парсим 260/1000\n",
            "Парсим 265/1000\n",
            "Парсим 270/1000\n",
            "Парсим 275/1000\n",
            "Парсим 280/1000\n",
            "Парсим 285/1000\n",
            "Парсим 290/1000\n",
            "Парсим 295/1000\n",
            "Парсим 300/1000\n",
            "Парсим 305/1000\n",
            "Парсим 310/1000\n",
            "Парсим 315/1000\n",
            "Парсим 320/1000\n",
            "Парсим 325/1000\n",
            "Парсим 330/1000\n",
            "Парсим 335/1000\n",
            "Парсим 340/1000\n",
            "Парсим 345/1000\n",
            "Парсим 350/1000\n",
            "Парсим 355/1000\n",
            "Парсим 360/1000\n",
            "Парсим 365/1000\n",
            "Парсим 370/1000\n",
            "Парсим 375/1000\n",
            "Парсим 380/1000\n",
            "Парсим 385/1000\n",
            "Парсим 390/1000\n",
            "Парсим 395/1000\n",
            "Парсим 400/1000\n",
            "Парсим 405/1000\n",
            "Парсим 410/1000\n",
            "Парсим 415/1000\n",
            "Парсим 420/1000\n",
            "Парсим 425/1000\n",
            "Парсим 430/1000\n",
            "Парсим 435/1000\n",
            "Парсим 440/1000\n",
            "Парсим 445/1000\n",
            "Парсим 450/1000\n",
            "Парсим 455/1000\n",
            "Парсим 460/1000\n",
            "Парсим 465/1000\n",
            "Парсим 470/1000\n",
            "Парсим 475/1000\n",
            "Парсим 480/1000\n",
            "Парсим 485/1000\n",
            "Парсим 490/1000\n",
            "Парсим 495/1000\n",
            "Парсим 500/1000\n",
            "Парсим 505/1000\n",
            "Парсим 510/1000\n",
            "Парсим 515/1000\n",
            "Парсим 520/1000\n",
            "Парсим 525/1000\n",
            "Парсим 530/1000\n",
            "Парсим 535/1000\n",
            "Парсим 540/1000\n",
            "Парсим 545/1000\n",
            "Парсим 550/1000\n",
            "Парсим 555/1000\n",
            "Парсим 560/1000\n",
            "Парсим 565/1000\n",
            "Парсим 570/1000\n",
            "Парсим 575/1000\n",
            "Парсим 580/1000\n",
            "Парсим 585/1000\n",
            "Парсим 590/1000\n",
            "Парсим 595/1000\n",
            "Парсим 600/1000\n",
            "Парсим 605/1000\n",
            "Парсим 610/1000\n",
            "Парсим 615/1000\n",
            "Парсим 620/1000\n",
            "Парсим 625/1000\n",
            "Парсим 630/1000\n",
            "Парсим 635/1000\n",
            "Парсим 640/1000\n",
            "Парсим 645/1000\n",
            "Парсим 650/1000\n",
            "Парсим 655/1000\n",
            "Парсим 660/1000\n",
            "Парсим 665/1000\n",
            "Парсим 670/1000\n",
            "Парсим 675/1000\n",
            "Парсим 680/1000\n",
            "Парсим 685/1000\n",
            "Парсим 690/1000\n",
            "Парсим 695/1000\n",
            "Парсим 700/1000\n",
            "Парсим 705/1000\n",
            "Парсим 710/1000\n",
            "Парсим 715/1000\n",
            "Парсим 720/1000\n",
            "Парсим 725/1000\n",
            "Парсим 730/1000\n",
            "Парсим 735/1000\n",
            "Парсим 740/1000\n",
            "Парсим 745/1000\n",
            "Парсим 750/1000\n",
            "Парсим 755/1000\n",
            "Парсим 760/1000\n",
            "Парсим 765/1000\n",
            "Парсим 770/1000\n",
            "Парсим 775/1000\n",
            "Парсим 780/1000\n",
            "Парсим 785/1000\n",
            "Парсим 790/1000\n",
            "Парсим 795/1000\n",
            "Парсим 800/1000\n",
            "Парсим 805/1000\n",
            "Парсим 810/1000\n",
            "Парсим 815/1000\n",
            "Парсим 820/1000\n",
            "Парсим 825/1000\n",
            "Парсим 830/1000\n",
            "Парсим 835/1000\n",
            "Парсим 840/1000\n",
            "Парсим 845/1000\n",
            "Парсим 850/1000\n",
            "Парсим 855/1000\n",
            "Парсим 860/1000\n",
            "Парсим 865/1000\n",
            "Парсим 870/1000\n",
            "Парсим 875/1000\n",
            "Парсим 880/1000\n",
            "Парсим 885/1000\n",
            "Парсим 890/1000\n",
            "Парсим 895/1000\n",
            "Парсим 900/1000\n",
            "Парсим 905/1000\n",
            "Парсим 910/1000\n",
            "Парсим 915/1000\n",
            "Парсим 920/1000\n",
            "Парсим 925/1000\n",
            "Парсим 930/1000\n",
            "Парсим 935/1000\n",
            "Парсим 940/1000\n",
            "Парсим 945/1000\n",
            "Парсим 950/1000\n",
            "Парсим 955/1000\n",
            "Парсим 960/1000\n",
            "Парсим 965/1000\n",
            "Парсим 970/1000\n",
            "Парсим 975/1000\n",
            "Парсим 980/1000\n",
            "Парсим 985/1000\n",
            "Парсим 990/1000\n",
            "Парсим 995/1000\n",
            "Парсим 1000/1000\n",
            "\n",
            "Успешно собрано 1000 объявлений\n",
            "\n",
            "Данные сохранены в parsingMOVE.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}