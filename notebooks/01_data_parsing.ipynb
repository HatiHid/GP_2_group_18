{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOLX7/wXpvVk6x7rEDwajXy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 01_data_parsing.ipynb\n",
        "\n",
        "В этом ноутбуке собираются данные о новостройках Москвы.\n",
        "\n",
        "Данные получены с помощью парсинга с сайта Move.ru.\n",
        "\n",
        "Результат ноутбука — таблица с сырыми объектами (цены, площади, адреса, девелопер и др.),\n",
        "которая используется в дальнейшей обработке и анализе.\n"
      ],
      "metadata": {
        "id": "uzefmx-7fT2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Парсинг данных с сайта Move.ru\n",
        "\n",
        "https://move.ru/kvartiry_v_novostroykah/v_predelah_mkad/\n",
        "\n"
      ],
      "metadata": {
        "id": "Al2U6X6ieifs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8aS8vb9egY3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "from fake_useragent import UserAgent\n",
        "ua = UserAgent()\n",
        "\n",
        "def get_page_with_retry(url, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            headers = {\n",
        "                'User-Agent': ua.random,\n",
        "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "                'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',\n",
        "                'Accept-Encoding': 'gzip, deflate, br',\n",
        "                'Connection': 'keep-alive',\n",
        "                'Upgrade-Insecure-Requests': '1',\n",
        "            }\n",
        "\n",
        "            response = requests.get(\n",
        "                url,\n",
        "                headers=headers,\n",
        "                timeout=10,\n",
        "                allow_redirects=True\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response\n",
        "            else:\n",
        "                print(f\"Попытка {attempt + 1}: Статус код {response.status_code} для {url}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Попытка {attempt + 1}: Ошибка при запросе {url}: {e}\")\n",
        "\n",
        "        if attempt < max_retries - 1:\n",
        "            time.sleep(random.uniform(2, 5))\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_urls_from_page(soup, base_url):\n",
        "    urls = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        if re.search(r'/objects/[\\w-]+\\s*[\\w\\s]*\\d+', href):\n",
        "            absolute_url = urljoin(base_url, href)\n",
        "            if absolute_url not in urls:\n",
        "                urls.append(absolute_url)\n",
        "    return urls\n",
        "\n",
        "def main():\n",
        "    base_url = 'https://move.ru/kvartiry_v_novostroykah/v_predelah_mkad/'\n",
        "    all_urls = []\n",
        "    max_pages = 400\n",
        "\n",
        "    for page_num in range(1, max_pages + 1):\n",
        "        print(f\"Обрабатывается страница {page_num} из {max_pages}...\")\n",
        "\n",
        "        if page_num == 1:\n",
        "            page_url = base_url\n",
        "        else:\n",
        "            page_url = f\"{base_url}?page={page_num}\"\n",
        "\n",
        "        # Получаем страницу\n",
        "        response = get_page_with_retry(page_url)\n",
        "\n",
        "        if response is None:\n",
        "            print(f\"Не удалось получить страницу {page_num}, пропускаем...\")\n",
        "            continue\n",
        "\n",
        "        # Парсим HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Извлекаем URL\n",
        "        page_urls = extract_urls_from_page(soup, base_url)\n",
        "        all_urls.extend(page_urls)\n",
        "\n",
        "        # Случайная задержка между запросами\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "\n",
        "    # Удаляем дубликаты\n",
        "    all_urls = list(set(all_urls))\n",
        "\n",
        "    # Сохраняем в файл\n",
        "    with open('extracted_urls.txt', 'w', encoding='utf-8') as f:\n",
        "        for url in all_urls:\n",
        "            f.write(url + '\\n')\n",
        "\n",
        "    print(f\"Результаты сохранены в файл 'extracted_urls.txt'\")\n",
        "\n",
        "if name == \"main\":\n",
        "    main()"
      ]
    }
  ]
}
