{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **01_data_parsing.ipynb**\n",
        "\n",
        "В этом ноутбуке собираются данные о недвижимости Москвы.\n",
        "\n",
        "Данные получены с помощью парсинга с сайта Move.ru.\n",
        "\n",
        "Результат ноутбука — таблица с сырыми объектами (цены, площади, адреса, застройщики и др.), которая используется в дальнейшей обработке и анализе."
      ],
      "metadata": {
        "id": "HBMZkjhyWCuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa3saglnhDWE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install requests beautifulsoup4 fake-useragent lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "from fake_useragent import UserAgent"
      ],
      "metadata": {
        "id": "U9ix8iHRGXTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала парсим главную страницу с предложенными объявлениями и смотрим где находятся ссылки, которые нам надо вычленить"
      ],
      "metadata": {
        "id": "dVeJuz3I5UwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://move.ru/kvartiry_v_novostroykah/v_predelah_mkad/'\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "5NPeMeBXGVCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "вычленяем эти самые ссылки с 400 страниц (в итоге вышло меньше, поскольку я по несколько раз продолжала парсить). На каждой странице по 30 ссылок"
      ],
      "metadata": {
        "id": "OANEkjSg5d4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "#Настройка браузера\n",
        "def setup_driver():\n",
        "    print(\"Настраиваем браузер\")\n",
        "\n",
        "    chrome_options = Options()  # Создание объекта для настройки опций Chrome\n",
        "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')  # Скрытие автоматизации\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])  # Отключение опций автоматизации\n",
        "\n",
        "    # Автоматическая установка драйвера\n",
        "    service = Service(ChromeDriverManager().install())  # Создание службы с автоматической установкой ChromeDriver\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)  # Создание экземпляра браузера Chrome\n",
        "\n",
        "    return driver\n",
        "\n",
        "def main():\n",
        "    driver = None  # Инициализация переменной драйвера (на случай ошибки)\n",
        "    try:\n",
        "        print(\"Запуск парсинга\")\n",
        "        driver = setup_driver()\n",
        "\n",
        "        base_url = 'https://move.ru/kvartiry_v_novostroykah/v_predelah_mkad/'\n",
        "        all_urls = []\n",
        "\n",
        "        for page_num in range(1, 400):\n",
        "            print(f\"Страница {page_num}\")\n",
        "\n",
        "            # Формируем URL\n",
        "            url = base_url if page_num == 1 else f\"{base_url}?page={page_num}\"  # Для первой страницы без параметра page\n",
        "            print(f\"Загружаем: {url}\")\n",
        "\n",
        "            # Переходим на страницу\n",
        "            driver.get(url)  # Команда браузеру перейти по указанному URL\n",
        "            time.sleep(3)\n",
        "\n",
        "            links = driver.find_elements(By.XPATH, \"//a[contains(@href, '/objects/')]\")  # Поиск всех ссылок содержащих '/objects/'\n",
        "\n",
        "            # Собираем URL\n",
        "            new_urls = []\n",
        "            for link in links:\n",
        "                href = link.get_attribute('href')\n",
        "                if href and href not in all_urls and href not in new_urls:\n",
        "                    new_urls.append(href)\n",
        "\n",
        "            all_urls.extend(new_urls)  # Добавление всех новых URL в общий список\n",
        "            print(f\"На странице {page_num} найдено {len(new_urls)} ссылок\")\n",
        "\n",
        "\n",
        "            wait_time = random.uniform(2, 4)\n",
        "            print(f\"Ждем {wait_time:.1f} секунд\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "\n",
        "        print(f\"Сохраняем {len(all_urls)} URL\")\n",
        "        with open('extracted_urls.txt', 'w', encoding='utf-8') as f:\n",
        "            for url in all_urls:\n",
        "                f.write(url + '\\n')\n",
        "\n",
        "        print(\"Результаты в файле 'extracted_urls.txt'\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка: {e}\")\n",
        "\n",
        "    finally:  # Блок который выполнится в любом случае\n",
        "        if driver:  # Проверка что драйвер существует\n",
        "            driver.quit()  # Закрытие браузера\n",
        "            print(\"Браузер закрыт\")\n",
        "\n",
        "if name == \"main\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "6Plx67XA3B-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Открываем сохраненный файл с ссылками"
      ],
      "metadata": {
        "id": "yenewzZv8eTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('extracted_urls.txt', 'r', encoding='utf-8') as file:\n",
        "    urls = [line.strip() for line in file if line.strip()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yDuo8HRucbi",
        "outputId": "4bb59247-d105-493c-c981-b5c541824374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найдено 10496 ссылок для парсинга\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь парсим одно из объявлений и смотрим его код, где находятся признаки, которые мы хотимм"
      ],
      "metadata": {
        "id": "tw7JNYFp8qV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://move.ru/objects/prodaetsya_2-komnatnaya_kvartira_ploschadyu_604_kvm_moskva_mojayskiy_rayon_ulica_vereyskaya_d_29s35_9285880157/'\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "QVyvF2qtTHqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "\n",
        "\n",
        "#Настройка браузера Chrome для парсинга\n",
        "def setup_driver():\n",
        "    print(\"Настраиваем браузер\")\n",
        "\n",
        "    chrome_options = Options() # Создаем объект для настроек Chrome\n",
        "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')  # Скрытие автоматизации\n",
        "    chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])  # Отключение опций автоматизации\n",
        "\n",
        "    # Автоматическая установка драйвера\n",
        "    service = Service(ChromeDriverManager().install())  # Создание службы с автоматической установкой ChromeDriver\n",
        "    driver = webdriver.Chrome(service=service, options=chrome_options)  # Создание экземпляра браузера Chrome\n",
        "\n",
        "    return driver\n",
        "\n",
        "regex_patterns = {\n",
        "    'id': re.compile(r'_(\\d+)/?$'),  # _ затем одна/несколько цифр, затем необязательный / и конец строки $\n",
        "    'price': re.compile(r'[\\d\\s\\xa0]+'), # один/несколько символов из класса: цифры, пробелы, неразрывные пробелы\n",
        "    'year': re.compile(r'(\\d{4})'), #точно 4 цифры подряд\n",
        "    'rooms': re.compile(r'(\\d+)'), # одна/несколько цифр\n",
        "    'area': re.compile(r'(\\d+(?:\\.\\d+)?)'), # цифры, затем необязательная незахватываемая группа (?:) с точкой и цифрами\n",
        "    'floor_current': re.compile(r'(\\d+)/\\d+'), # цифры, затем /, затем цифры\n",
        "    'floor_total': re.compile(r'/(\\d+)'), # символ /, затем цифры\n",
        "    'views': re.compile(r'(\\d+)'), # одна/несколько цифр\n",
        "    'distance_km': re.compile(r'([\\d,]+)\\s*км'), # цифры/запятые, затем пробелы, затем текст км\n",
        "    'distance_m': re.compile(r'(\\d+)\\s*м') # цифры, затем пробелы, затем текст м\n",
        "}\n",
        "\n",
        "def parse_property_page(driver, url, i, urls):\n",
        "    try:\n",
        "        if i % 5 == 0:\n",
        "            print(f\"Парсим {i}/{len(urls)}\")\n",
        "\n",
        "        driver.get(url)\n",
        "\n",
        "        # Ждем загрузки основных элементов страницы (максимум 10 секунд)\n",
        "        WebDriverWait(driver, 10).until(\n",
        "            EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "        )\n",
        "\n",
        "        time.sleep(2)\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
        "\n",
        "        data = {}\n",
        "\n",
        "        # 1. Извлекаем ID из URL\n",
        "        id_match = regex_patterns['id'].search(url)\n",
        "        data['id'] = id_match.group(1) if id_match else None\n",
        "\n",
        "        # 2. Парсим цену\n",
        "        try:\n",
        "            price_elem = driver.find_element(By.CLASS_NAME, 'card-objects-price__main-price')\n",
        "            price_text = price_elem.text.strip()\n",
        "\n",
        "            # Очищаем текст цены от лишних символов\n",
        "            clean_price = re.sub(r'[^\\d,]', '', price_text.replace('\\xa0', ''))\n",
        "            clean_price = clean_price.replace(',', '.')\n",
        "\n",
        "            if clean_price:\n",
        "                price = float(clean_price)\n",
        "                data['price_millions'] = round(price / 1000000, 2)\n",
        "        except:\n",
        "            data['price_millions'] = None\n",
        "# 3-5. Парсим основную информацию: название ЖК, застройщик, класс жилья\n",
        "        try:\n",
        "            spec_items = driver.find_elements(By.CLASS_NAME, 'card-specifications-table__item')\n",
        "            count = 0\n",
        "\n",
        "            for item in spec_items:\n",
        "                # Если нашли все три характеристики, прерываем цикл\n",
        "                if count == 3:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    item_text = item.text\n",
        "\n",
        "                    # Название ЖК\n",
        "                    if 'Название ЖК' in item_text:\n",
        "                        try:\n",
        "                            link_elem = item.find_element(By.CLASS_NAME, 'card-specifications-table__link')\n",
        "                            data['complex_name'] = link_elem.text.strip()\n",
        "                            count += 1\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    # Застройщик\n",
        "                    elif 'Застройщик' in item_text:\n",
        "                        try:\n",
        "                            link_elem = item.find_element(By.CLASS_NAME, 'card-specifications-table__link')\n",
        "                            data['developer'] = link_elem.text.strip()\n",
        "                            count += 1\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    # Класс жилья\n",
        "                    elif 'Класс жилья' in item_text:\n",
        "                        try:\n",
        "                            title_elem = item.find_element(By.CLASS_NAME, 'card-specifications-table__title')\n",
        "                            data['housing_class'] = title_elem.text.strip()\n",
        "                            count += 1\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 6-12. Парсим технические характеристики: площади, этажи, комнаты\n",
        "        try:\n",
        "            table_items = driver.find_elements(By.CLASS_NAME, 'card-specifications-table__item')\n",
        "            count = 0\n",
        "\n",
        "            for item in table_items:\n",
        "                # Если нашли все 5 характеристик, прерываем цикл\n",
        "                if count == 5:\n",
        "                    break\n",
        "\n",
        "                try:\n",
        "                    item_text = item.text\n",
        "\n",
        "                    # Общая площадь\n",
        "                    if 'Общая площадь' in item_text and 'total_area' not in data:\n",
        "                        match = regex_patterns['area'].search(item_text)\n",
        "                        if match:\n",
        "                            data['total_area'] = float(match.group(1))\n",
        "                            count += 1\n",
        "\n",
        "                    # Жилая площадь\n",
        "                    elif 'Жилая площадь' in item_text and 'living_area' not in data:\n",
        "                        match = regex_patterns['area'].search(item_text)\n",
        "                        if match:\n",
        "                            data['living_area'] = float(match.group(1))\n",
        "                            count += 1\n",
        "\n",
        "                    # Площадь кухни\n",
        "                    elif 'Площадь кухни' in item_text and 'kitchen_area' not in data:\n",
        "                        match = regex_patterns['area'].search(item_text)\n",
        "                        if match:\n",
        "                            data['kitchen_area'] = float(match.group(1))\n",
        "                            count += 1\n",
        "\n",
        "                    # Этаж\n",
        "                    elif 'Этаж' in item_text and 'floor' not in data:\n",
        "                        match = regex_patterns['floor_current'].search(item_text)\n",
        "                        if match:\n",
        "                            data['floor'] = int(match.group(1))\n",
        "\n",
        "                        match = regex_patterns['floor_total'].search(item_text)\n",
        "                        if match:\n",
        "                            data['total_floors'] = int(match.group(1))\n",
        "\n",
        "                        count += 1\n",
        "\n",
        "                    # Количество комнат\n",
        "elif 'Количество комнат' in item_text and 'rooms' not in data:\n",
        "                        match = regex_patterns['rooms'].search(item_text)\n",
        "                        if match:\n",
        "                            data['rooms'] = int(match.group(1))\n",
        "                            count += 1\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 13. Парсим полный адрес\n",
        "        try:\n",
        "            address_link = driver.find_element(By.CLASS_NAME, 'base-link.card-objects-location__address-link')\n",
        "            data['full_address'] = address_link.get_attribute('title') or address_link.text.strip()\n",
        "        except:\n",
        "            data['full_address'] = None\n",
        "\n",
        "        # 14-17. Парсим информацию о метро\n",
        "        try:\n",
        "            metro_stations = driver.find_elements(By.CLASS_NAME, 'card-objects-near-stations__station')\n",
        "            metro_data = []\n",
        "\n",
        "            for station in metro_stations:\n",
        "                try:\n",
        "                    # Название станции метро\n",
        "                    name_elem = station.find_element(By.CLASS_NAME, 'card-objects-near-stations__station-link')\n",
        "                    metro_name = name_elem.text.strip()\n",
        "\n",
        "                    # Расстояние до метро\n",
        "                    distance_m = None\n",
        "                    try:\n",
        "                        distance_elem = station.find_element(By.CLASS_NAME, 'card-objects-near-stations__station-distance')\n",
        "                        distance_text = distance_elem.text.strip()\n",
        "\n",
        "                        # Парсим километры\n",
        "                        km_match = regex_patterns['distance_km'].search(distance_text)\n",
        "                        if km_match:\n",
        "                            distance_m = int(float(km_match.group(1).replace(',', '.')) * 1000)\n",
        "                        else:\n",
        "                            # Парсим метры\n",
        "                            m_match = regex_patterns['distance_m'].search(distance_text)\n",
        "                            if m_match:\n",
        "                                distance_m = int(m_match.group(1))\n",
        "\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                    metro_data.append({\n",
        "                        'name': metro_name,\n",
        "                        'distance_m': distance_m\n",
        "                    })\n",
        "\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "            # Сортируем станции метро по расстоянию и берем ближайшую\n",
        "            if metro_data:\n",
        "                metro_data.sort(key=lambda x: x['distance_m'] if x['distance_m'] else float('inf'))\n",
        "                nearest = metro_data[0]\n",
        "\n",
        "                data['metro_data'] = metro_data\n",
        "                data['metro_names'] = [m['name'] for m in metro_data]\n",
        "                data['nearest_metro'] = nearest['name']\n",
        "                data['nearest_metro_distance'] = nearest['distance_m']\n",
        "\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 18. Парсим количество просмотров\n",
        "        try:\n",
        "            views_elems = driver.find_elements(By.CLASS_NAME, 'card-meta__item')\n",
        "            for elem in views_elems:\n",
        "                views_text = elem.text.strip()\n",
        "                if 'просмотр' in views_text.lower():\n",
        "                    views_match = regex_patterns['views'].search(views_text)\n",
        "                    if views_match:\n",
        "                        data['views'] = int(views_match.group(1))\n",
        "                        break\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        data['url'] = url\n",
        "\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при парсинге {url}: {e}\")\n",
        "        return None\n",
        "def main():\n",
        "    driver = None\n",
        "    all_data = []\n",
        "\n",
        "    try:\n",
        "        # Настраиваем браузер\n",
        "        driver = setup_driver()\n",
        "        print(\"Браузер запущен, начинаем парсинг\")\n",
        "\n",
        "        # Читаем URL из файла\n",
        "        with open('extracted_urls.txt', 'r', encoding='utf-8') as f:\n",
        "            urls = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "        print(f\"Найдено {len(urls)} URL для парсинга\")\n",
        "\n",
        "        # Парсим каждое объявление\n",
        "        for i, url in enumerate(urls, 1):\n",
        "            data = parse_property_page(driver, url, i, urls)\n",
        "            if data:\n",
        "                all_data.append(data)\n",
        "\n",
        "            time.sleep(random.uniform(0.5, 1.5))\n",
        "\n",
        "            # Дополнительная пауза каждые 20 объявлений\n",
        "            if i % 20 == 0:\n",
        "                sleep_time = random.uniform(2, 4)\n",
        "                print(f\"Пауза {sleep_time:.1f} секунд\")\n",
        "                time.sleep(sleep_time)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Критическая ошибка: {e}\")\n",
        "\n",
        "    finally:\n",
        "        # Закрываем браузер в любом случае\n",
        "        if driver:\n",
        "            driver.quit()\n",
        "            print(\"Браузер закрыт\")\n",
        "\n",
        "    # Создаем DataFrame и сохраняем результаты\n",
        "    if all_data:\n",
        "        df = pd.DataFrame(all_data)\n",
        "        print(f\"\\nУспешно собрано {len(df)} объявлений\")\n",
        "\n",
        "        df.to_csv('parsingMOVE.csv', index=False, encoding='utf-8')\n",
        "        print(\"Данные сохранены в parsingMOVE.csv\")\n",
        "\n",
        "    else:\n",
        "        print(\"Не удалось собрать данные\")\n",
        "\n",
        "if name == \"main\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "E60xLuXA9G3X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}