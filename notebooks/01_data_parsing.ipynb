{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **01_data_parsing.ipynb**\n",
        "\n",
        "В этом ноутбуке собираются данные о недвижимости Москвы.\n",
        "\n",
        "Данные получены с помощью парсинга с сайта Move.ru.\n",
        "\n",
        "Результат ноутбука — таблица с сырыми объектами (цены, площади, адреса, девелопер и др.), которая используется в дальнейшей обработке и анализе."
      ],
      "metadata": {
        "id": "HBMZkjhyWCuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa3saglnhDWE",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "pip install requests beautifulsoup4 fake-useragent lxml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import random\n",
        "import logging\n",
        "from fake_useragent import UserAgent"
      ],
      "metadata": {
        "id": "U9ix8iHRGXTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала парсим главную страницу с предложенными объявлениями и смотрим где находятся ссылки, которые нам надо вычленить"
      ],
      "metadata": {
        "id": "dVeJuz3I5UwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://move.ru/kvartiry_v_novostroykah/v_predelah_mkad/'\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "5NPeMeBXGVCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "вычленяем эти самые ссылки с 400 страниц (в итоге вышло меньше, поскольку я по несколько раз продолжала парсить). На каждой странице по 30 ссылок"
      ],
      "metadata": {
        "id": "OANEkjSg5d4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем экземпляр UserAgent для генерации случайных заголовков браузера\n",
        "ua = UserAgent()\n",
        "\n",
        "def get_page_with_retry(url, max_retries=3):\n",
        "\n",
        "    # Цикл по количеству попыток\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            # Формируем заголовки HTTP-запроса\n",
        "            headers = {\n",
        "                'User-Agent': ua.random, # Случайный User-Agent\n",
        "                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', # Типы контента, которые принимаем\n",
        "                'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3', # Предпочтительные языки\n",
        "                'Accept-Encoding': 'gzip, deflate, br', # Поддерживаемые методы сжатия\n",
        "                'Connection': 'keep-alive', # Поддержание соединения\n",
        "                'Upgrade-Insecure-Requests': '1', # Запрос на обновление небезопасных запросов\n",
        "            }\n",
        "            # Выполняем GET-запрос с заданными параметрами\n",
        "            response = requests.get(\n",
        "                url,\n",
        "                headers=headers,\n",
        "                timeout=10,\n",
        "                allow_redirects=True # Разрешаем перенаправления\n",
        "            )\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response\n",
        "            else:\n",
        "                print(f\"Попытка {attempt + 1}: Статус код {response.status_code} для {url}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Попытка {attempt + 1}: Ошибка при запросе {url}: {e}\")\n",
        "\n",
        "        if attempt < max_retries - 1:\n",
        "            time.sleep(random.uniform(2, 5))\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_urls_from_page(soup, base_url):\n",
        "    urls = []\n",
        "    for link in soup.find_all('a', href=True):\n",
        "        href = link['href']\n",
        "        if re.search(r'/objects/[\\w-]+\\s*[\\w\\s]*\\d+', href):\n",
        "            if href not in urls:\n",
        "                urls.append(href)\n",
        "    return urls\n",
        "\n",
        "def main():\n",
        "    base_url = 'https://move.ru/kvartiry_v_novostroykah/v_predelah_mkad/'\n",
        "    all_urls = []\n",
        "    max_pages = 400\n",
        "\n",
        "    for page_num in range(1, max_pages + 1):\n",
        "        print(f\"Обрабатывается страница {page_num} из {max_pages}...\")\n",
        "\n",
        "        # Формируем URL страницы (для первой страницы без параметра page)\n",
        "        if page_num == 1:\n",
        "            page_url = base_url\n",
        "        else:\n",
        "            page_url = f\"{base_url}?page={page_num}\"\n",
        "\n",
        "        # Получаем страницу\n",
        "        response = get_page_with_retry(page_url)\n",
        "\n",
        "        if response is None:\n",
        "            print(f\"Не удалось получить страницу {page_num}, пропускаем...\")\n",
        "            continue\n",
        "\n",
        "        # Парсим HTML\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Извлекаем URL\n",
        "        page_urls = extract_urls_from_page(soup, base_url)\n",
        "        all_urls.extend(page_urls)\n",
        "\n",
        "        time.sleep(random.uniform(1, 3))\n",
        "\n",
        "    all_urls = list(set(all_urls))\n",
        "\n",
        "    with open('extracted_urls.txt', 'w', encoding='utf-8') as f:\n",
        "        for url in all_urls:\n",
        "            f.write(url + '\\n')\n",
        "\n",
        "    print(f\"Результаты сохранены в файл 'extracted_urls.txt'\")\n",
        "\n",
        "if name == \"main\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "6Plx67XA3B-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Открываем сохраненный файл с ссылками"
      ],
      "metadata": {
        "id": "yenewzZv8eTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('extracted_urls.txt', 'r', encoding='utf-8') as file:\n",
        "    urls = [line.strip() for line in file if line.strip()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yDuo8HRucbi",
        "outputId": "4bb59247-d105-493c-c981-b5c541824374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Найдено 10496 ссылок для парсинга\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь парсим одно из объявлений и смотрим его код, где находятся признаки, которые мы хотимм"
      ],
      "metadata": {
        "id": "tw7JNYFp8qV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://move.ru/objects/prodaetsya_2-komnatnaya_kvartira_ploschadyu_604_kvm_moskva_mojayskiy_rayon_ulica_vereyskaya_d_29s35_9285880157/'\n",
        "page = requests.get(url)\n",
        "soup = BeautifulSoup(page.text, 'html.parser')\n",
        "print(soup.prettify())"
      ],
      "metadata": {
        "id": "QVyvF2qtTHqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Создаем сессию для сохранения cookies и заголовков между запросами\n",
        "session = requests.Session()\n",
        "# Устанавливаем стандартные заголовки для имитации браузера\n",
        "session.headers.update({\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "})\n",
        "\n",
        "# Предкомпилируем регулярные выражения\n",
        "regex_patterns = {\n",
        "    'id': re.compile(r'_(\\d+)/?$'),  # _ затем одна/несколько цифр, затем необязательный / и конец строки $\n",
        "    'price': re.compile(r'[\\d\\s\\xa0]+'), # один/несколько символов из класса: цифры, пробелы, неразрывные пробелы\n",
        "    'year': re.compile(r'(\\d{4})'), #точно 4 цифры подряд\n",
        "    'rooms': re.compile(r'(\\d+)'), # одна/несколько цифр\n",
        "    'area': re.compile(r'(\\d+(?:\\.\\d+)?)'), # цифры, затем необязательная незахватываемая группа (?:) с точкой и цифрами\n",
        "    'floor_current': re.compile(r'(\\d+)/\\d+'), # цифры, затем /, затем цифры\n",
        "    'floor_total': re.compile(r'/(\\d+)'), # символ /, затем цифры\n",
        "    'views': re.compile(r'(\\d+)'), # одна/несколько цифр\n",
        "    'distance_km': re.compile(r'([\\d,]+)\\s*км'), # цифры/запятые, затем пробелы, затем текст км\n",
        "    'distance_m': re.compile(r'(\\d+)\\s*м') # цифры, затем пробелы, затем текст м\n",
        "}\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for i, url in enumerate(urls, 1):\n",
        "    try:\n",
        "        if i%5 == 0:\n",
        "            print(f\"Парсим {i}/{len(urls)}\")\n",
        "\n",
        "        # Выполняем GET-запрос к странице объявления\n",
        "        response = session.get(url, timeout=8)\n",
        "        # Парсим HTML-контент с помощью BeautifulSoup и lxml парсера\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "        data = {}\n",
        "\n",
        "        # 1. Айди\n",
        "        id_match = regex_patterns['id'].search(url)\n",
        "        data['id'] = id_match.group(1) if id_match else None\n",
        "\n",
        "        # 2. Цена в миллионах\n",
        "        price_elem = soup.find('span', class_='card-objects-price__main-price')\n",
        "        if price_elem:\n",
        "            price_text = price_elem.get_text(strip=True)\n",
        "            # Убираем все нецифровые символы кроме точек и запятых\n",
        "            clean_price = re.sub(r'[^\\d,]', '', price_text.replace('\\xa0', ''))\n",
        "            clean_price = clean_price.replace(',', '.')\n",
        "            if clean_price:\n",
        "                try:\n",
        "                    price = float(clean_price)\n",
        "                    data['price_millions'] = round(price / 1000000, 2)\n",
        "                except ValueError:\n",
        "                    print(f\"Не удалось преобразовать цену: {price_text}\")\n",
        "\n",
        "        # 3-5. Название ЖК + застройщик + класс жилья\n",
        "        spec_items = soup.find_all('div', class_='card-specifications-table__item')\n",
        "        count = 0\n",
        "        for item in spec_items:\n",
        "            desc_elem = item.find('span', class_='card-specifications-table__description')\n",
        "            value_elem = item.find('span', class_='card-specifications-table__title')\n",
        "            link_elem = item.find('a', class_='card-specifications-table__link')\n",
        "\n",
        "            # если мы нашли все три признака заканчиваем и не обрабатываем дальше\n",
        "            if count == 3:\n",
        "                  break\n",
        "\n",
        "            if desc_elem:\n",
        "                desc_text = desc_elem.get_text(strip=True)\n",
        "\n",
        "                # Название ЖК\n",
        "                if desc_text == 'Название ЖК':\n",
        "                  data['complex_name'] = link_elem.get_text(strip=True)\n",
        "                  count += 1\n",
        "\n",
        "                # Застройщик\n",
        "                elif desc_text == 'Застройщик':\n",
        "                    data['developer'] = link_elem.get_text(strip=True)\n",
        "                    count += 1\n",
        "\n",
        "                # Класс жилья\n",
        "                elif desc_text == 'Класс жилья':\n",
        "                    data['housing_class'] = value_elem.get_text(strip=True)\n",
        "                    count += 1\n",
        "\n",
        "\n",
        "        # 6-12. Основные данные (площади, этажи, комнаты)\n",
        "        table_items = soup.find_all('div', class_='card-specifications-table__item')\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        for item in table_items:\n",
        "            desc_elem = item.find('span', class_='card-specifications-table__description')\n",
        "            title_elem = item.find('span', class_='card-specifications-table__title')\n",
        "\n",
        "             # если мы нашли все пять признаков, заканчиваем и не обрабатываем дальше\n",
        "            if count == 5:\n",
        "              break\n",
        "\n",
        "            if desc_elem and title_elem:\n",
        "                desc = desc_elem.get_text(strip=True)\n",
        "                title = title_elem.get_text(strip=True)\n",
        "\n",
        "                if 'Общая площадь' in desc and 'total_area' not in data:\n",
        "                    match = regex_patterns['area'].search(title)\n",
        "                    if match:\n",
        "                        data['total_area'] = float(match.group(1))\n",
        "                        count += 1\n",
        "                elif 'Жилая площадь' in desc:\n",
        "                    match = regex_patterns['area'].search(title)\n",
        "                    if match:\n",
        "                        data['living_area'] = float(match.group(1))\n",
        "                        count += 1\n",
        "                elif 'Площадь кухни' in desc:\n",
        "                    match = regex_patterns['area'].search(title)\n",
        "                    if match:\n",
        "                        data['kitchen_area'] = float(match.group(1))\n",
        "                        count += 1\n",
        "                elif 'Этаж' in desc and 'floor' not in data:\n",
        "                    match = regex_patterns['floor_current'].search(title)\n",
        "                    if match:\n",
        "                        data['floor'] = int(match.group(1))\n",
        "                    match = regex_patterns['floor_total'].search(title)\n",
        "                    if match:\n",
        "                        data['total_floors'] = int(match.group(1))\n",
        "                    count += 1\n",
        "                elif 'Количество комнат' in desc and 'rooms' not in data:\n",
        "                    match = regex_patterns['rooms'].search(title)\n",
        "                    if match:\n",
        "                        data['rooms'] = int(match.group(1))\n",
        "                        count += 1\n",
        "\n",
        "        # 13. Полный адрес\n",
        "        address_link = soup.find('a', class_='base-link card-objects-location__address-link')\n",
        "        data['full_address'] = address_link['title'] if address_link and address_link.get('title') else None\n",
        "\n",
        "        # 14-17. Метро\n",
        "        metro_data = []\n",
        "        metro_stations = soup.find_all('li', class_='card-objects-near-stations__station')\n",
        "\n",
        "        for station in metro_stations:\n",
        "            name_elem = station.find('a', class_='card-objects-near-stations__station-link')\n",
        "            duration_elem = station.find('span', class_='card-objects-near-stations__station-duration')\n",
        "            distance_elem = station.find('span', class_='card-objects-near-stations__station-distance')\n",
        "\n",
        "            if name_elem:\n",
        "                metro_name = name_elem.get_text(strip=True)\n",
        "\n",
        "                # Парсим расстояние\n",
        "                distance_m = None\n",
        "                if distance_elem:\n",
        "                    distance_text = distance_elem.get_text(strip=True)\n",
        "                    km_match = regex_patterns['distance_km'].search(distance_text)\n",
        "                    if km_match:\n",
        "                        distance_m = int(float(km_match.group(1).replace(',', '.')) * 1000) # group(1) это то, что было захвачено в первых скобках ([\\d,]+) регулярного выражения\n",
        "                    else:\n",
        "                        m_match = regex_patterns['distance_m'].search(distance_text)\n",
        "                        if m_match:\n",
        "                            distance_m = int(m_match.group(1))\n",
        "\n",
        "                metro_data.append({\n",
        "                    'name': metro_name,\n",
        "                    'distance_m': distance_m\n",
        "                })\n",
        "\n",
        "        # Сортируем по расстоянию и берем ближайшее\n",
        "        if metro_data:\n",
        "            metro_data.sort(key=lambda x: x['distance_m'] if x['distance_m'] else float('inf'))\n",
        "            nearest = metro_data[0]\n",
        "\n",
        "            data['metro_data'] = metro_data\n",
        "            data['metro_names'] = [m['name'] for m in metro_data]\n",
        "            data['nearest_metro'] = nearest['name']\n",
        "            data['nearest_metro_distance'] = nearest['distance_m']\n",
        "\n",
        "        # 18. Просмотры\n",
        "        views_elems = soup.find_all('span', class_='card-meta__item')\n",
        "        for elem in views_elems:\n",
        "            views_text = elem.get_text(strip=True)\n",
        "            if 'просмотр' in views_text.lower():\n",
        "                views_match = regex_patterns['views'].search(views_text)\n",
        "                if views_match:\n",
        "                    data['views'] = int(views_match.group(1))\n",
        "                    break\n",
        "\n",
        "        data['url'] = url\n",
        "        all_data.append(data)\n",
        "        time.sleep(random.uniform(0.1, 0.5))\n",
        "        if i % 100 == 0:\n",
        "            time.sleep(random.uniform(1.5, 2))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка при парсинге {url}: {e}\")\n",
        "        continue\n",
        "\n",
        "# Создаем DataFrame\n",
        "df = pd.DataFrame(all_data)\n",
        "\n",
        "print(f\"\\nУспешно собрано {len(df)} объявлений\")\n",
        "\n",
        "# Сохраняем в CSV\n",
        "df.to_csv('parsingMOVE.csv', index=False, encoding='utf-8')\n",
        "print(\"\\nДанные сохранены в parsingMOVE.csv\")"
      ],
      "metadata": {
        "id": "E60xLuXA9G3X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}